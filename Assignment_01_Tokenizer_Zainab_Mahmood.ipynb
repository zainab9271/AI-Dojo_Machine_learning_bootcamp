{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 01: Tokenizer - Zainab Mahmood.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainab9271/AI-Dojo_Machine_learning_bootcamp/blob/main/Assignment_01_Tokenizer_Zainab_Mahmood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 01: Tokenizer \n",
        "\n",
        "What is Tokenization in NLP? Tokenization is one of the most common tasks when it comes to working with text data. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens then we can use the tokenizer to convert text to vector of integer numbers.\n",
        "\n",
        "In this week's assignment we will create a tokenizer using Python by following the instructions below:\n",
        "\n",
        "\n",
        "\n",
        "1.   Create a Python class called `Tokenizer`.\n",
        "2.   The class construction (`__init__`) takes in one parameter called `raw_text` which is the text corpus. Use the text corpus to create two dictionaries .... .\n",
        "3. Create a class method that return two dictionaries `index_word` which assigns a unique number as key to each word, `word_index` which is the reverse of `index_word` (i.e., the word in the key and the number is the value).\n",
        "4. Create a class method called `tokenize` to that takes a parameter `text` and converts it to list of integers using the `word_index` dictionary.\n",
        "5. Create a class method called `reverse_tokenize` to that takes a parameter `vector` and converts it to a string using the `index_word` dictionary.\n",
        "6. Download your Colab file by going to `File > Download > Download .ipynb` and send the file along with your email using [this form](https://forms.gle/JrPDmELeef5vfr7MA).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DUDhNK7QOOfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Text File \n",
        "\n",
        "When working interactively with the standard Python interpreter, one of the frustrations is the need to switch between multiple windows to access Python tools and system command-line tools.\n",
        "\n",
        "IPython bridges this gap, and gives you a syntax for executing shell commands directly from within the IPython terminal.\n",
        "\n",
        "The magic happens with the exclamation point: anything appearing after ``!`` on a line will be executed not by the Python kernel, but by the system command-line.\n",
        "\n",
        "Since Google Colab is built on Linux we can execute Linux commands in Colab and one of the commands to retrieve datasets is wget. wget stands for ‘web get’ and using this command will retrieve the dataset directly from the source.\n",
        "\n"
      ],
      "metadata": {
        "id": "XAHqJPlbgm-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/TheAIDojo/Machine_Learning_Bootcamp/main/Week%2001-%20%20Introduction%20to%20Python/text_data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly8yQ4rwgzLp",
        "outputId": "2bac0e9e-1347-4b76-8caf-6bb5f8b9d71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-18 07:48:11--  https://raw.githubusercontent.com/TheAIDojo/Machine_Learning_Bootcamp/main/Week%2001-%20%20Introduction%20to%20Python/text_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11916 (12K) [text/plain]\n",
            "Saving to: ‘text_data.txt’\n",
            "\n",
            "\rtext_data.txt         0%[                    ]       0  --.-KB/s               \rtext_data.txt       100%[===================>]  11.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-18 07:48:11 (91.6 MB/s) - ‘text_data.txt’ saved [11916/11916]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the Text File \n"
      ],
      "metadata": {
        "id": "P8i7ekH3O5_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data from the text file \n",
        "with open('/content/text_data.txt','r') as f:\n",
        "    row_text=f.read()"
      ],
      "metadata": {
        "id": "6SPs5CAMPKJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Tokenizer Class "
      ],
      "metadata": {
        "id": "LshIeWf1OpXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        row_text,\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "        \"\"\"\n",
        "        The __init__ method  of the class Tokenizer will get the following parameters\n",
        "        row_text : the input text that we wnat to create the tokenizer from it.\n",
        "        filters : a string where each element is a character that will be filtered from the texts. \n",
        "        The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
        "         \"\"\"\n",
        "\n",
        "        self.row_text = row_text\n",
        "        self.filters = filters\n",
        "\n",
        "        # create the word to index and index to word dictionaries using the tokenizer method\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        self.word_index, self.index_word = self.tokenizer(self.row_text)\n",
        "        \n",
        "        # end of the code\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        The clean_text method will take one parameter text \n",
        "        then the method will clean the text data \n",
        "        and return clean text\n",
        "        \"\"\"\n",
        "\n",
        "        text = text.lower()  # convert the texts to lowercase.\n",
        "\n",
        "        # clean the text\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for char in self.filters:\n",
        "            text = text.replace(char, ' ')\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "        return text  # return the clean text\n",
        "\n",
        "    def tokenizer(self, text):\n",
        "        \"\"\"\n",
        "        The tokenizer method will take one parameter text \n",
        "        then the method will clean the text data. \n",
        "        extricate the unique words the method will return two dictionaries contains the unique word to index and the reverse of it.\n",
        "        \"\"\"\n",
        "\n",
        "        # create the word to index dictionary\n",
        "\n",
        "        word_index = {}\n",
        "\n",
        "        # create the index to word dictionary\n",
        "\n",
        "        index_word = {}\n",
        "\n",
        "       \n",
        "\n",
        "        # clean the text\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        text = self.clean_text(text) \n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # extract the unique values\n",
        "\n",
        "        # start of the code\n",
        "        \n",
        "        unique_words = list(set(text.split()))\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # convert the word to index and the reverse of it\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for index, word in enumerate(unique_words):\n",
        "            word_index[word] = index\n",
        "            index_word[index] = word\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "        # return word to index and the indeex to word dictionaries\n",
        "\n",
        "        return (word_index, index_word)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        the method tokenize will convert text to vector of integers  \n",
        "        text: the text paramater it's for the text that you want to convert to list of numbers\n",
        "        \"\"\"\n",
        "\n",
        "        # create empty  list\n",
        "\n",
        "        vector = []\n",
        "\n",
        "\n",
        "\n",
        "        # clean the text\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        text = self.clean_text(text) \n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "        # split the text to words\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        words_list = text.split() \n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "        # convert the word list to\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for word in words_list:\n",
        "            vector.append(self.word_index[word]) \n",
        "\n",
        "        # end of the code\n",
        "            \n",
        "\n",
        "        # return the  vector\n",
        "\n",
        "        return vector\n",
        "\n",
        "    def reverse_tokenize(self, vector):\n",
        "        \"\"\"\n",
        "        the method reverse_tokenize will convert vector of integers to text\n",
        "        vector: the vector paramater it's for the list of number that you want to convert to string\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        word_list = []\n",
        "\n",
        "        # convert from tokenize to string\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for index in vector:\n",
        "            word_list.append(self.index_word[index])\n",
        "\n",
        "        text = \" \".join(word_list)\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "        # return the string\n",
        "\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "kPbBX2ZTOiHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tokenizer object \n",
        "\n",
        "tokenizer = Tokenizer(row_text)"
      ],
      "metadata": {
        "id": "wjKqhKD4WQad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "22lRp7kFWVi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Your Tokenizer"
      ],
      "metadata": {
        "id": "4Qj-QcZpCWmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_1 = \"\"\"\n",
        "Care for us! True, indeed! They ne'er cared for us\n",
        "yet: suffer us to famish, and their store-houses\n",
        "crammed with grain; make edicts for usury, to\n",
        "support usurers; repeal daily any wholesome act\n",
        "established against the rich, and provide more\n",
        "piercing statutes daily, to chain up and restrain\n",
        "the poor. If the wars eat us not up, they will; and\n",
        "there's all the love they bear us.\"\"\"\n",
        "\n",
        "test_2 = \"\"\"Well, I'll hear it, sir: yet you must not think to\n",
        "fob off our disgrace with a tale: but, an 't please\n",
        "you, deliver.\"\"\""
      ],
      "metadata": {
        "id": "pyG0ccsXWX2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_1 = tokenizer.tokenize(test_1)\n",
        "print(tokens_1)\n",
        "text_1 = tokenizer.reverse_tokenize(tokens_1)\n",
        "print(text_1)"
      ],
      "metadata": {
        "id": "cL6xYV4OC6Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27cd663-2a7d-4071-d2c9-dcb741ab56fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[298, 553, 281, 523, 656, 428, 292, 165, 553, 281, 660, 573, 281, 507, 695, 0, 369, 614, 519, 512, 88, 598, 704, 257, 553, 244, 507, 503, 354, 287, 626, 605, 723, 148, 690, 445, 340, 220, 0, 593, 580, 51, 348, 626, 507, 291, 270, 0, 193, 340, 370, 685, 340, 156, 516, 281, 128, 270, 428, 249, 0, 403, 728, 340, 347, 428, 221, 281]\n",
            "care for us true indeed they ne'er cared for us yet suffer us to famish and their store houses crammed with grain make edicts for usury to support usurers repeal daily any wholesome act established against the rich and provide more piercing statutes daily to chain up and restrain the poor if the wars eat us not up they will and there's all the love they bear us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_2 = tokenizer.tokenize(test_2)\n",
        "print(tokens_2)\n",
        "text_2 = tokenizer.reverse_tokenize(tokens_2)\n",
        "print(text_2)"
      ],
      "metadata": {
        "id": "Wsgy7TceFn_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f472cf8b-de56-4735-9973-93342e58c89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26, 441, 157, 561, 310, 660, 371, 404, 128, 106, 507, 115, 279, 214, 1, 88, 360, 218, 201, 154, 226, 42, 371, 744]\n",
            "well i'll hear it sir yet you must not think to fob off our disgrace with a tale but an 't please you deliver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected output for test_1 \n",
        "\n",
        "\n",
        "```\n",
        "[470, 615, 112, 308, 715, 152, 392, 184, 615, 112, 27, 557, 112, 564, 752, 300, 217, 524, 691, 386, 622, 590, 432, 367, 615, 500, 564, 254, 145, 241, 416, 108, 459, 344, 703, 692, 220, 740, 300, 578, 686, 156, 402, 416, 564, 497, 421, 300, 455, 220, 682, 55, 220, 603, 382, 112, 723, 421, 152, 19, 300, 700, 360, 220, 559, 152, 287, 112]\n",
        "care for us true indeed they ne'er cared for us yet suffer us to famish and their store houses crammed with grain make edicts for usury to support usurers repeal daily any wholesome act established against the rich and provide more piercing statutes daily to chain up and restrain the poor if the wars eat us not up they will and there's all the love they bear us\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The expected output for test_2\n",
        "\n",
        "\n",
        "```\n",
        "[272, 167, 580, 366, 707, 27, 95, 154, 723, 115, 564, 111, 663, 529, 531, 622, 435, 350, 523, 179, 643, 633, 95, 609]\n",
        "well i'll hear it sir yet you must not think to fob off our disgrace with a tale but an 't please you deliver\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aUESU0DCFPPc"
      }
    }
  ]
}